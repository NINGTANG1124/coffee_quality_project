# -*- coding: utf-8 -*-
"""1.15

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GztWwqy7UXhuRoPMeDiZ5kCMYC6FrNnb
"""

from google.colab import drive; drive.mount('/content/drive')

import pandas as pd

#1. 读取文件
data = pd.read_csv('/content/drive/MyDrive/coffee_quality_project/coffee_ratings.csv') #提交作业时候记得更改

# 2. 检查缺失值
print("缺失值统计：")
print(data.isnull().sum())

# 删除缺失值过多的列（>50%）
missing_percentage = data.isnull().mean() * 100
columns_to_drop = missing_percentage[missing_percentage > 50].index
data = data.drop(columns=columns_to_drop)
#这些列通常很难提供可靠的分析结果，缺失数据过多的列可能会对模型的性能造成负面影响，因为模型无法有效利用这些信息。但有列很重要，缺失数据可以通过其他变量推断（如 altitude_mean_meters 可通过平均高度填充），可以选择保留。

# 填充缺失值：数值列用中位数，类别列用众数
data['total_cup_points'].fillna(data['total_cup_points'].median(), inplace=True)
data['country_of_origin'].fillna(data['country_of_origin'].mode()[0], inplace=True)
data['species'].fillna(data['species'].mode()[0], inplace=True)

# 3. 处理重复值
duplicates = data.duplicated().sum()
print(f"重复值数量: {duplicates}")
data = data.drop_duplicates()

# 4. 数据类型标准化--已经放在对应的图里面了
# 将字符串标准化
data['country_of_origin'] = data['country_of_origin'].str.strip().str.title()
data['species'] = data['species'].str.strip().str.capitalize()

# 转换数值列为浮点类型
data[['altitude_mean_meters', 'total_cup_points']] = data[['altitude_mean_meters', 'total_cup_points']].apply(pd.to_numeric, errors='coerce')

# 5. 异常值处理
# 剔除评分低于50或高于100的异常值
data = data[(data['total_cup_points'] >= 50) & (data['total_cup_points'] <= 100)]

# 剔除海拔值为负数的异常样本
data = data[data['altitude_mean_meters'] > 0]

# 6. 删除无关列
columns_to_remove = ['lot_number', 'ico_number', 'certification_body', 'expiration']
data = data.drop(columns=columns_to_remove, errors='ignore')

# 7. 输出清理后的数据概况
print("清理后的数据概况：")
print(data.info())
print(data.describe())

# 8. 保存清理后的数据
data.to_csv("cleaned_coffee_data.csv", index=False)

import pandas as pd

# 加载清理后的数据文件
file_path = "/content/cleaned_coffee_data.csv"  # 确认文件路径是否正确
cleaned_data = pd.read_csv(file_path)

# 检查数据是否加载成功
print(cleaned_data.info())

#3.数据可视化
import seaborn as sns
import matplotlib.pyplot as plt

# 使用清理后的数据
data = cleaned_data  # 使用清理后的数据集

# 绘制评分分布直方图
# Overall Rating Distribution (0-100)
plt.figure(figsize=(10, 6))
sns.histplot(data['total_cup_points'], bins=50, kde=True, color='blue')
plt.title("Overall Rating Distribution (0-100)")
plt.xlabel("Total Cup Points")
plt.ylabel("Sample Count")
plt.xlim(0, 100)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

data['species'] = data['species'].str.strip().str.capitalize()
# 检查咖啡种类
unique_species = cleaned_data['species'].unique()
species_count = cleaned_data['species'].nunique()

print("咖啡种类的唯一值有：", unique_species)
print("咖啡种类的总数是：", species_count)

# 绘制不同种类（species）的评分分布
species_list = cleaned_data['species'].unique()

plt.figure(figsize=(12, 8))
for species in species_list:
    subset = cleaned_data[cleaned_data['species'] == species]
    sns.histplot(subset['total_cup_points'], kde=True, label=species, bins=30, alpha=0.5)

plt.title('Total Cup Points Distribution by Species', fontsize=16)
plt.xlabel('Total Cup Points', fontsize=12)
plt.ylabel('Sample Count', fontsize=12)
plt.legend(title='Species')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# 使用清理后的数据
data = cleaned_data

# 1. 散点图：评分与海拔
plt.figure(figsize=(10, 6))
sns.scatterplot(x='altitude_mean_meters', y='total_cup_points', data=data, alpha=0.6, color='blue')
plt.title('Scatter Plot: Total Cup Points vs Altitude', fontsize=16)
plt.xlabel('Mean Altitude (meters)', fontsize=12)
plt.ylabel('Total Cup Points', fontsize=12)
plt.grid()
plt.show()

# 2. 回归分析
plt.figure(figsize=(10, 6))
sns.regplot(x='altitude_mean_meters', y='total_cup_points', data=data, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})
plt.title('Regression: Total Cup Points vs Altitude', fontsize=16)
plt.xlabel('Mean Altitude (meters)', fontsize=12)
plt.ylabel('Total Cup Points', fontsize=12)
plt.grid()
plt.show()

# 检查国家名称的唯一值
print("Unique countries in the data:")
print(data['country_of_origin'].unique())

# 标准化国家名称（去除多余空格并首字母大写）
data['country_of_origin'] = data['country_of_origin'].str.strip().str.title()

import plotly.express as px

# 1. 准备数据：按国家计算评分的平均值
country_ratings = data.groupby('country_of_origin', as_index=False)['total_cup_points'].mean()
country_ratings.columns = ['country', 'avg_rating']

# 检查数据
print(country_ratings.head())

# 2. 绘制交互式地图
fig = px.choropleth(
    country_ratings,
    locations="country",  # 与地图匹配的国家名称
    locationmode="country names",  # 使用国家名称进行匹配
    color="avg_rating",  # 以评分平均值为颜色变量
    title="Average Coffee Rating by Country",
    color_continuous_scale="YlGnBu",  # 颜色方案
    labels={'avg_rating': 'Average Rating'}  # 图例标签
)

fig.show()

# 计算相关性矩阵
correlation_matrix = data[['total_cup_points', 'flavor', 'acidity']].corr()

# 绘制热力图
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap: Ratings, Flavor, and Acidity', fontsize=16)
plt.show()